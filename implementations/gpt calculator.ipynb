{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11f671e-f2e5-43d4-a032-da808a0d52e7",
   "metadata": {},
   "source": [
    "GPT Calculator\n",
    "---\n",
    "\n",
    "In this notebook, I'll implement a basic calculator using a decoder-only transformer (GPT like) with binary operators (for instance `+`, `-`, `*`, `/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbc32c5-e28a-4e9d-9fa9-1f040f2bbc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import haiku as hk\n",
    "\n",
    "import einops\n",
    "from relax.models.gpt import GPT\n",
    "from relax import Trainer, TrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb43fe2-ea55-4ad9-b758-5be5602f8b4f",
   "metadata": {},
   "source": [
    "### Create our own naive character based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4621c9a7-ef3c-4315-878b-10f7dff93b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer <vocab_size: 1, eot: None>\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.char_to_token_dict : dict = {}\n",
    "        self.__eot : str = None\n",
    "        self.__altered : bool = False\n",
    "    \n",
    "    def add_char(self, char: str) -> None:\n",
    "        self.char_to_token_dict[char] = self.vocab_size\n",
    "        self.__altered = True\n",
    "    \n",
    "    def encode_char(self, char: str) -> int:\n",
    "        try:\n",
    "            return self.char_to_token_dict[char]\n",
    "        except KeyError as error:\n",
    "            raise ValueError(f\"Character `{char}` not contained in the vocabulary.\") from error\n",
    "    \n",
    "    def decode_indice(self, indice: int) -> str:\n",
    "        try:\n",
    "            return self.token_to_char_dict[indice]\n",
    "        except KeyError as error:\n",
    "            raise ValueError(f\"Indice `{indice}` not contained in the vocabulary.\") from error\n",
    "    \n",
    "    def encode(self, string: str) -> list:\n",
    "        return [self.encode_char(c) for c in string]\n",
    "    \n",
    "    def decode(self, indices: Union[list, jnp.ndarray], ignore : list = None, stop_after_eot : bool = False) -> list:\n",
    "        if ignore is None:\n",
    "            ignore : list = []\n",
    "        if hasattr(indices, 'tolist'):\n",
    "            indices = indices.tolist()\n",
    "        raw_decoded: str = ''.join([self.decode_indice(indice) for indice in indices if indice not in ignore])\n",
    "        return raw_decoded.split(self.eot)[0] + self.eot if stop_after_eot and self.eot in raw_decoded else raw_decoded\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.char_to_token_dict)\n",
    "    \n",
    "    @property\n",
    "    def token_to_char_dict(self):\n",
    "        @lru_cache(maxsize=1)\n",
    "        def lru_inner():\n",
    "            return {token:char for char, token in self.char_to_token_dict.items()}\n",
    "        if self.__altered:\n",
    "            lru_inner.cache_clear()\n",
    "        return lru_inner()\n",
    "    \n",
    "    @property\n",
    "    def eot(self) -> str:\n",
    "        return self.__eot\n",
    "    \n",
    "    @property\n",
    "    def eot_token(self) -> int:\n",
    "        return self.encode_char(self.eot)\n",
    "    \n",
    "    @eot.setter\n",
    "    def eot(self, value: str):\n",
    "        if not value in self.char_to_token_dict:\n",
    "            self.add_char(value)\n",
    "        self.__eot = value\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Tokenizer <vocab_size: {self.vocab_size}, eot: {self.eot}>\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "        \n",
    "tokenizer = Tokenizer()\n",
    "string : str = 'a' * 10\n",
    "tokenizer.add_char('a')\n",
    "assert string == tokenizer.decode(tokenizer.encode(string))\n",
    "print(tokenizer)\n",
    "del tokenizer, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2c5b01-ff1f-46f1-b3d6-b53a2e6d2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def model(x):\n",
    "    return GPT(\n",
    "        vocab_size = 20,\n",
    "        block_size = 32,\n",
    "        n_blocks = 2,\n",
    "        n_embed = 32,\n",
    "        n_head = 2,\n",
    "        dropout_rate = 0.2,\n",
    "    )(x)\n",
    "\n",
    "def softmax_cross_entropy(logits, labels, ignore_index=None):\n",
    "    # vmap the actual cross entropy function per batch input - go brrrrr\n",
    "    @jax.vmap\n",
    "    def batch_ce(logits, oh_labels):\n",
    "        return -jnp.sum(jax.nn.log_softmax(logits) * oh_labels)\n",
    "        \n",
    "    # Get the mask in order to filter out the non desired indexes (usually pad tokens for text)\n",
    "    mask = labels != ignore_index if ignore_index != None else jnp.ones_like(labels, dtype=bool)\n",
    "    # Turn the labels into one hot format\n",
    "    one_hot = hk.one_hot(labels, logits.shape[-1])\n",
    "    # Get the cross entropy per batch entry\n",
    "    bloss = batch_ce(logits, one_hot)\n",
    "    # And aggregate all desired entries\n",
    "    return jnp.mean(bloss, where=mask)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, rng, data) -> jnp.ndarray:\n",
    "    inputs, labels = data\n",
    "    logits = model.apply(params, rng, inputs)\n",
    "    # Flatten the batch dim and sequence dim for both the logits and labels\n",
    "    logits = einops.rearrange(logits, '... d -> (...) d')\n",
    "    labels = einops.rearrange(labels, '... -> (...)')\n",
    "    # Get the batch + sequences losses\n",
    "    bloss = softmax_cross_entropy(logits, labels, ignore_index=0)\n",
    "    return jnp.mean(bloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ac9f73-ac68-4ba7-b108-d8eb79b6b318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, main_key, cardinality : int, minimum : int, maximum : int, operations : list[str], sequence_length : int, pad_token : int):\n",
    "    def get_sample(key):\n",
    "        ops = {\n",
    "            '+' : '__add__',\n",
    "            '*' : '__mul__',\n",
    "            '-' : '__sub__',\n",
    "            '/' : '__div__',\n",
    "            }\n",
    "        \n",
    "        # Get the keys for choosing the 2 integers and the operation\n",
    "        k0, k1 = jax.random.split(key, 2)\n",
    "        a, b = jax.random.randint(k0, (2,), minimum, maximum)\n",
    "        op = operations[jax.random.choice(k1, jnp.arange(len(operations)))]\n",
    "        \n",
    "        x = f\"{a.item()} {op} {b.item()} = \"\n",
    "        result = f\"{(getattr(a, ops[op])(b)).item()}\"\n",
    "        \n",
    "        # Encode both the input and the output that is `input + result + eot`\n",
    "        input_tokens = tokenizer.encode(x)\n",
    "        output_tokens = input_tokens + tokenizer.encode(result) + [tokenizer.eot_token]\n",
    "        \n",
    "        # Then pad the sequences\n",
    "        input_tokens += [pad_token] * (sequence_length - len(input_tokens))\n",
    "        output_tokens += [pad_token] * (sequence_length - len(output_tokens))\n",
    "        \n",
    "        return input_tokens, output_tokens\n",
    "    \n",
    "    keys = jax.random.split(main_key, cardinality)\n",
    "    return jnp.array([get_sample(key) for key in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df589fa2-a468-44e3-a8c1-56bb2104427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer <vocab_size: 19, eot: ;>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'p': 0,\n",
       " '0': 1,\n",
       " '1': 2,\n",
       " '2': 3,\n",
       " '3': 4,\n",
       " '4': 5,\n",
       " '5': 6,\n",
       " '6': 7,\n",
       " '7': 8,\n",
       " '8': 9,\n",
       " '9': 10,\n",
       " '+': 11,\n",
       " '-': 12,\n",
       " '*': 13,\n",
       " '/': 14,\n",
       " '=': 15,\n",
       " '.': 16,\n",
       " ' ': 17,\n",
       " ';': 18}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "chars = ['p'] # Naive padding character, can also use only -1 when creating the dataset\n",
    "chars += list(range(10)) # The ten integers \n",
    "chars += list('+-*/=. ') # The 4 operations, equal sign, the dot for floats and space for nice formatting\n",
    "for char in chars:\n",
    "    tokenizer.add_char(str(char))\n",
    "tokenizer.eot = ';' # Then, use semicolons for the end of text token\n",
    "\n",
    "print(tokenizer)\n",
    "tokenizer.char_to_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03baf0b5-4015-40be-9ceb-8cafd7b4af4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2, 64, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = get_dataset(\n",
    "    tokenizer,\n",
    "    jax.random.PRNGKey(42),\n",
    "    10240, # Get 1024 observations\n",
    "    1,  \n",
    "    100,\n",
    "    list('+-'),\n",
    "    sequence_length = 32,\n",
    "    pad_token = tokenizer.encode_char('p')\n",
    ")\n",
    "\n",
    "# Get batched dataset\n",
    "ds = einops.rearrange(entries, '(b bs) xy s -> b xy bs s', bs=64)\n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0510e6e-5aad-406b-9bcd-f2cb92f86458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of calculations : \n",
      "\t5 + 25 = -> 5 + 25 = 30;\n",
      "\t4 - 10 = -> 4 - 10 = -6;\n",
      "\t46 + 94 = -> 46 + 94 = 140;\n",
      "\t89 + 70 = -> 89 + 70 = 159;\n",
      "\t42 + 38 = -> 42 + 38 = 80;\n",
      "\t42 + 73 = -> 42 + 73 = 115;\n",
      "\t95 - 34 = -> 95 - 34 = 61;\n",
      "\t81 - 85 = -> 81 - 85 = -4;\n",
      "\t18 - 96 = -> 18 - 96 = -78;\n",
      "\t9 + 45 = -> 9 + 45 = 54;\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of calculations : \")\n",
    "for x, y in einops.rearrange(ds[0, :, :10, :], 'xy b s -> b xy s'):\n",
    "    print(f\"\\t{tokenizer.decode(x, ignore=[0])}-> {tokenizer.decode(y, ignore=[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6702b92c-a895-4b19-8adc-bcf7f0644940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = TrainingConfig(\n",
    "            epochs=1000,\n",
    "            )\n",
    "\n",
    "optimizer = optax.adam(0.001)\n",
    "\n",
    "trainer = Trainer(model, optimizer, config)\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "fake_input = jnp.zeros((1, 32), dtype=int)\n",
    "init_state = trainer.init(rng, fake_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e47a78df-25bb-4218-ab14-b2f715b081f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42906d6ed4f44b3dbc1998c2fbe01dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_state = trainer.train(init_state, loss_fn, ds, jit_update_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38a62059-f7c1-440b-bfbe-f77ce03929b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of outputs:\n",
      "x -> y_hat (y)\n",
      "\t5 + 25 = -> 5 + 25 = 30; (5 + 25 = 30;)\n",
      "\t4 - 10 = -> 4 - 10 = -1; (4 - 10 = -6;)\n",
      "\t46 + 94 = -> 46 + 94 = 140; (46 + 94 = 140;)\n",
      "\t89 + 70 = -> 89 + 70 = 159; (89 + 70 = 159;)\n",
      "\t42 + 38 = -> 42 + 38 = 80; (42 + 38 = 80;)\n",
      "\t42 + 73 = -> 42 + 73 = 115; (42 + 73 = 115;)\n",
      "\t95 - 34 = -> 95 - 34 = 61; (95 - 34 = 61;)\n",
      "\t81 - 85 = -> 81 - 85 = -4; (81 - 85 = -4;)\n",
      "\t18 - 96 = -> 18 - 96 = -78; (18 - 96 = -78;)\n",
      "\t9 + 45 = -> 9 + 45 = 64; (9 + 45 = 54;)\n"
     ]
    }
   ],
   "source": [
    "x, y = ds[0]\n",
    "key = jax.random.PRNGKey(0)\n",
    "logits = model.apply(trained_state.params, key, x)\n",
    "y_hat = jnp.argmax(jax.nn.log_softmax(logits, -1), -1)\n",
    "\n",
    "print(\"Example of outputs:\")\n",
    "print(\"x -> y_hat (y)\")\n",
    "for (x_, y_), y_hat_ in zip(einops.rearrange(ds[0, :, :10, :], 'xy b s -> b xy s'), y_hat):\n",
    "    print(f\"\\t{tokenizer.decode(x_, ignore=[0])}-> {tokenizer.decode(y_hat_, ignore=[0], stop_after_eot=True)} ({tokenizer.decode(y_, ignore=[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8332ae7-f40f-4457-84b8-33aa1328eeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossentropy loss : 0.07728936523199081\n",
      "Accuracy on train set : 0.69208984375\n"
     ]
    }
   ],
   "source": [
    "x, y = einops.rearrange(ds, 'b xy bs s -> xy (b bs) s')\n",
    "loss = loss_fn(trained_state.params, jax.random.PRNGKey(0), (x, y))\n",
    "logits = model.apply(trained_state.params, jax.random.PRNGKey(0), x)\n",
    "y_hat = jnp.argmax(jax.nn.log_softmax(logits, -1), -1)\n",
    "y_str : list[str] = [tokenizer.decode(indices, ignore=[0], stop_after_eot=True) for indices in y]\n",
    "y_hat_str : list[str] = [tokenizer.decode(indices, ignore=[0], stop_after_eot=True) for indices in y_hat]\n",
    "print(f\"Crossentropy loss : {loss}\")\n",
    "print(f\"Accuracy on train set : {sum([a == b for a, b in zip(y_str, y_hat_str)]) / len(y_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a738aed-f5d2-46db-80b4-f1b9c1f5a81b",
   "metadata": {},
   "source": [
    "#### Try it on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41b07530-1b95-482d-a019-7027ae64771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = get_dataset(\n",
    "    tokenizer,\n",
    "    jax.random.PRNGKey(43),\n",
    "    128, # Get 128 observations\n",
    "    1,  \n",
    "    100,\n",
    "    list('+-'),\n",
    "    sequence_length = 32,\n",
    "    pad_token = tokenizer.encode_char('p')\n",
    ")\n",
    "x, y = val_ds = einops.rearrange(val_ds, 'a b ... -> b a ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04bfb926-edac-4a8a-9d3f-38223e14a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossentropy loss : 0.09523070603609085\n",
      "Accuracy on val set : 0.6484375\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(trained_state.params, jax.random.PRNGKey(0), (x, y))\n",
    "logits = model.apply(trained_state.params, jax.random.PRNGKey(0), x)\n",
    "y_hat = jnp.argmax(jax.nn.log_softmax(logits, -1), -1)\n",
    "y_str : list[str] = [tokenizer.decode(indices, ignore=[0], stop_after_eot=True) for indices in y]\n",
    "y_hat_str : list[str] = [tokenizer.decode(indices, ignore=[0], stop_after_eot=True) for indices in y_hat]\n",
    "print(f\"Crossentropy loss : {loss}\")\n",
    "print(f\"Accuracy on val set : {sum([a == b for a, b in zip(y_str, y_hat_str)]) / len(y_str)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepjax",
   "language": "python",
   "name": "deepjax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
